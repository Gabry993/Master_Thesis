\paragraph{ROS Library:} changing the library for ROS, using both official and unofficial;
\paragraph{Wheel Mode vs Joint Mode:} those servos have the possibility to directly control their velocity (wheel mode) and not their position (joint mode, default behaviour). Thus, we implemented an open loop controller in wheel mode;
\paragraph{PID Controller:} again using wheel mode, we tried to build a PID controller;
\paragraph{PID, Joint Mode:} we even tried a PID controller in joint mode, though it did not make much sense;
\paragraph{Compliance Slope and Margin:} those servos have two internal parameters. \emph{Compliance} is to set the control flexibility of the motor.
Diagram in figure \ref{fig:compliance} shows the relationship between output torque and position of the motor. \emph{Compliance Margin} exists in each direction of CW/CCW and means the error between goal position and present position. The greater the value, the more difference occurs. \emph{Compliance Slope} sets the level of Torque near the goal position, the higher the value, the more flexibility is obtained.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/compliance.png}%
	\caption{Compliance Slope and Margin}
	\label{fig:compliance}
\end{figure}
\\


DEMO SOFTWARE IMPLEMENTATION OLD

\section{Demos Implementation}
We have implemented three different demos to show the behaviour of the system and also demonstrate possible use cases. In that section we will discuss only aspects related to the software implementation of those demos. They are presented in depth in \ref{chap:4} 
\subsection{Relloc Demos}
Obviously, those demos are needed to give an example of the relloc. There are 2 different version of it. In both cases, the turret is placed on a surface (e.g. a table) and the human can stay on any point on the floor.\\ The first one computes the relloc between the human and the turret and then uses the result obtained to mark the point where the user is standing with the laser. This is useful to have a qualitative idea of the precision of the relloc.\\
The second version again computes the relloc, but then allows the user to directly control the laser dot position on the floor (we implemented also a wall version) by pointing at it. This, of course, is possible thanks to the relloc obtained. That demos is very interesting because it gives us a good insight of the entire system: the turret, the pointing model and the relloc.\\
All the transformations needed between the turret and the human frame are easily implemented thanks to the \textbf{tf} library for ROS, which played a major role for those demos. All the code which run those examples is launched inside \textbf{mx64\_try.launch}. In particular, the core part of the demos is inside \textbf{relloc\_try.py}.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/rellocDemo.png}%
	\caption{User Trying the Relloc Demo}
	\label{fig:rellocDemo}
\end{figure}
\subsection{Kobuki Go to Goal Demo}\label{subs:kobukiGoToGoal}
That demo exploits the kobuki platform to show an interesting use case for the system: mobile robot navigation.\\
A video which shows the demo is available online.\\
First the user does the relloc (figure \ref{fig:goToGoalDemo}, top-left corner), then he can control the laser (figure \ref{fig:goToGoalDemo}, top-right corner). After that, he orders the kobuki to reach a certain point by pointing at it with the laser for three seconds (figure \ref{fig:goToGoalDemo}, bottom-left corner). Finally, the kobuki goes there while keeping the laser pointed at the goal position (figure \ref{fig:goToGoalDemo}, bottom-right corner).\\
On a software level, we implemented a basic PID controller to move the kobuki. The library to interface with the robot is available online \cite{kobuki:ROS}.\\ To detect the fact that the user is pointing the same point for three seconds, we implemented a nice function with a fixed size queue where we store the coordinates of the last 150 laser points (which means points in the last $3$ seconds at $50Hz$). We check if the average distance of each point from the first point in the queue is within a given threshold: in that case we detect the 3 seconds pointing.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/goToGoalDemo.png}%
	\caption{Kobuki Go to Goal Demo}
	\label{fig:goToGoalDemo}
\end{figure}
\subsection{Kobuki Follow Trajectory Demo}
In that case we have the same ingredients of the \emph{Go to Goal Demo}, but the last part is different. Now the kobuki will not simply go to a goal point, but rather it will follow a trajectory drawn by the user. So, he would point a location for three second until a sound feedback is emitted. That marks the begin of the trajectory. Then he will draw the entire trajectory with the laser and stop again for three seconds at the end of the trajectory. Another sound is played and the kobuki will reach the start of the trajectory and then will follow it until its last point. A video is available also for that demo.\\
The three second pointing detection code is the same explained in \ref{subs:kobukiGoToGoal}. Moreover, we added code to sample the trajectory point list based on distance: that means that the actual trajectory followed by the kobuki is composed of points which are at a threshold distance from their predecessor. Of course, if that threshold is zero, we are using all the sampled points.\\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/followTrajectoryDemo.png}%
	\caption{Kobuki Follow Trajecotry Demo}
	\label{fig:followTrajectoryDemo}
\end{figure}




MOUNTING INSTRUCTION
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/axMounting1.png}%
	\caption{Mounting F2 Frame}
	\label{fig:axMounting1}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/axMounting2.png}%
	\caption{Mounting F3 Frame}
	\label{fig:axMounting2}
\end{figure}